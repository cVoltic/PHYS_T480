{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Distributions and Estimators\n",
    "\n",
    "G. Richards, 2016\n",
    "\n",
    "Resources for this material include Ivezic Sections 3.2-3.5, Karen' Leighly's [Bayesian Statistics Lecture](http://seminar.ouml.org/lectures/bayesian-statistics/), and Bevington's book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Our goal is ultimately to figure out the *distribution* from which our data is drawn, i.e., we want to know the *model*.  For example, let's say that we are trying to characterize the population of asteroids in the Solar System.  Maybe their sizes have a Gaussian distribution (with some characteristic size), or maybe they have a flat distribution (with equal numbers over a large range of sizes).  Or maybe the distribution is a power-law, with lots of little asteroids and very few big ones.  Or maybe it is a power-law in the other direction: very few little ones and lots of big ones.  If you are the first person to discover asteroids, then *you don't know*.  Our job is to figure that out: based entirely on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "That leads us to the need for **estimators**.  Since we don't know the distribution, we have to estimate it.  \n",
    "\n",
    "So, the book spends a lot of time talking about estimators and possible distributions.  \n",
    "\n",
    "Let's first review some commonly computed statistical properties of a data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "from astroML import stats as astroMLstats\n",
    "data = np.random.random(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The **arithmetic mean** (or Expectation value) is\n",
    "\n",
    "$$\\mu = E(x) = \\int_{-\\infty}^{\\infty} x h(x) dx,$$\n",
    "\n",
    "where $h(x)$ must be properly normalized and the integral gets replaced by a sum for discrete distributions.\n",
    "\n",
    "Specifically, this is the expecation value of $x$.  If you want the expectation value of something else--say $x^2$ or $(x-\\mu)^2$, you replace $x$ with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "mean = np.mean(data)\n",
    "print mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "While it is perhaps most common to compute the mean, the median is a more *robust* estimator of the (true) mean location of the distribution.  That's because it is less affected by outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Execute this cell.  Think about what it is doing.\n",
    "median = np.median(data)\n",
    "mask = data>0.75\n",
    "data[mask] = data[mask]*2\n",
    "newmedian = np.median(data)\n",
    "newmean = np.mean(data)\n",
    "print median,newmedian\n",
    "print mean,newmean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In addition to the \"average\", we'd like to know something about **deviations** from the average.  The simplest thing to compute is $$d_i = x_i - \\mu.$$  However, the average deviation is zero by definition of the mean.  The next simplest thing to do is to compute the mean absolute deviation:\n",
    "$$\\frac{1}{N}\\sum|x_i-\\mu|,$$\n",
    "but the absolute values can hide the true scatter of the distribution [in some cases](http://www.mathsisfun.com/data/standard-deviation.html).  So the next simplest thing to do is to square the differences $$\\sigma^2 = \\frac{1}{N}\\sum(x_i-\\mu)^2,$$ which we call the **variance**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Indeed the *variance* is just expectation value of $(x-\\mu)^2$\n",
    "\n",
    "$$\\sigma^2 = V = \\int_{-\\infty}^{\\infty}  (x-\\mu)^2 h(x) dx,$$\n",
    "\n",
    "where, again,  the integral gets replaced by a sum for discrete distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "var = np.var(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And we define the **standard deviation** as\n",
    "$$\\sigma = \\sqrt{V}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "std = np.std(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also the Median Absolute Deviation (MAD) given by\n",
    "$${\\rm median} (|x_i-{\\rm median}(\\{x_i\\})|)$$\n",
    "where $\\sigma = 1.4826\\,{\\rm MAD}$ for a Gaussian distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Percentiles, $q_p$, are computed as\n",
    "$$\\frac{p}{100} = \\int_{-\\infty}^{q_p}h(x) dx$$\n",
    "\n",
    "For example, the 25th, 50th, and 75th percentiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Complete this cell and execute\n",
    "q25,q50,q75 = # Complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Where we call the difference between the 25th and 75th percentiles, $q_{75} - q_{25}$, the *interquartile range*.\n",
    "\n",
    "The median and interquartile range are more _robust_ than the mean and standard deviation.  So, one can create a standard deviation like measurement (at least for a Gaussian) from the interquartile range as\n",
    "$\\sigma_G = 0.7413(q_{75} - q_{25})$, which we saw last time.  One reason to use this is the same as for the median.  $\\sigma_G$ is a more *robust* estimator of the scale of the distribution.  The normalization makes it *unbiased* for a perfect Gaussian (more on that later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "astroMLstats.sigmaG(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The mode is the most probable value, determined from the peak of the distribution, which is the value where the derivative is 0:\n",
    "$$ \\left(\\frac{dh(x)}{dx}\\right)_{x_m} = 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "mode = scipy.stats.mode(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Another way to estimate the mode (at least for a Gaussian distribution) is\n",
    "$$x_m = 3q_{50} - 2\\mu$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "modealt = 3*q50 - 2*mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Other useful measures include the \"higher order\" moments (the skewness and kurtosis):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "skew = scipy.stats.skew(data)\n",
    "kurt = scipy.stats.kurtosis(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Excute this cell\n",
    "print mean, median, var, std, skew, kurt, mode.mode, modealt, q25, q50, q75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We could do the same with a normal distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Complete and Execute this cell\n",
    "ndata = # Make this a normal distribution with mean=0, sigma=1 with a sample size of 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute all the above stats for this distribution\n",
    "print np.mean(ndata), np.median(ndata), np.var(ndata), np.std(ndata)\n",
    "print scipy.stats.skew(ndata), scipy.stats.kurtosis(ndata), scipy.stats.mode(ndata).mode\n",
    "print np.percentile(ndata, [25,50,75])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sample vs. Population Statistics \n",
    "\n",
    "Statistics estimated from the *data* are called _sample statistics_ as compared to _population statistics_ which come from knowing the functional form of the pdf.  Up to now we have been computing population statistics.\n",
    "\n",
    "Specifically, $\\mu$ is the *population average*, i.e., it is the expecation value of $x$ for $h(x)$.  But we don't *know* $h(x)$.  So the **sample mean**, $\\overline{x}$, is an *estimator* of $\\mu$, defined as\n",
    "$$\\overline{x} \\equiv \\frac{1}{N}\\sum_{i=1}^N x_i,$$\n",
    "which we determine from the data itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Then instead of $\\sigma^2$, which is the population variance, we have the **sample variance**, $s^2$, where\n",
    "\n",
    "$$s^2 = \\frac{1}{N-1}\\sum_{i=1}^N(x_i-\\overline{x})^2$$\n",
    "\n",
    "Where it is $N-1$ instead of $N$ since we had to determine $\\overline{x}$ from the data instead of using a known $\\mu$.  Ideally one tries to work in a regime where $N$ is large enough that we can be lazy and ignore this. \n",
    "\n",
    "So the mean and variance of a distribution are $\\mu$ and $\\sigma^2$.  The *estimators* of the distribution are $\\overline{x}$ (or $\\hat{x}$) and $s^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bias\n",
    "\n",
    "If there is a difference between the *estimator* and the *population* values, we say that the estimator is **biased** (perhaps not quite the usage of the word that you are used to).  Again, more on this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Uncertainty\n",
    "\n",
    "We would also like to know the uncertainty of our estimates $\\overline{x}$ and $s$.  Note that $s$ is **NOT** the uncertainty of $\\overline{x}$.  Rather the uncertainty of $\\overline{x}$, $\\sigma_{\\overline{x}}$ is \n",
    "$$ \\sigma_{\\overline{x}} = \\frac{s}{\\sqrt{N}},$$\n",
    "which we call the *standard error of the mean*.\n",
    "\n",
    "The uncertainty of $s$ itself is\n",
    "$$\\sigma_s = \\frac{s}{\\sqrt{2(N-1)}} = \\frac{1}{\\sqrt{2}}\\sqrt{\\frac{N}{N-1}}\\sigma_{\\overline{x}}.$$\n",
    "\n",
    "Note that for large $N$, $\\sigma_{\\overline{x}} \\sim \\sqrt{2}\\sigma_s$ and for small $N$, $\\sigma_s$ is not much smaller than $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Distributions\n",
    "\n",
    "If we are attempting to characterize our data in a way that is **parameterized**, then we need a functional form or a **distribution**.  There are many naturally occurring distributions.  The book goes through quite a few of them.  Here we'll just talk about a few basic ones to get us started.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Uniform Distribution\n",
    "\n",
    "The uniform distribution is perhaps more commonly called a \"top-hat\" or a \"box\" distribution.  It is specified by a mean, $\\mu$, and a width, $W$, where\n",
    "\n",
    "$$p(x|\\mu,W) = \\frac{1}{W}$$\n",
    "\n",
    "over the range $|x-\\mu|\\le \\frac{W}{2}$ and $0$ otherwise.  That says that \"given $\\mu$ AND $W$, the probability of $x$ is $\\frac{1}{W}$\" (as long as we are within a certain range).\n",
    "\n",
    "Since we are used to thinking of a Gaussian as the *only* type of distribution the concept of $\\sigma$ (aside from the width) may seem strange.  But $\\sigma$ as mathematically defined above applies here and\n",
    "$$\\sigma = \\frac{W}{\\sqrt{12}}.$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "%matplotlib inline\n",
    "%run code/fig_uniform_distribution.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can implement [uniform](http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.uniform.html#scipy.stats.uniform) in `scipy` as follows.  Use the methods listed at the bottom of the link to complete the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Complete and execute this cell\n",
    "from scipy import stats\n",
    "dist = # Complete for left edge = 0, width = 2\n",
    "r = dist # Complete for 10 random draws\n",
    "print r\n",
    "p = dist # Complete for pdf evaluated at x=1\n",
    "print p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Did you expect that answer for the pdf?  Why?  What would the pdf be if you changed the width to 4?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gaussian Distribution\n",
    "\n",
    "We have already seen that the Gaussian distribution is given by\n",
    "$$p(x|\\mu,\\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(\\frac{-(x-\\mu)^2}{2\\sigma^2}\\right).$$\n",
    "\n",
    "It is also called the **normal distribution** and can be noted by $\\mathscr{N}(\\mu,\\sigma)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "%run code/fig_gaussian_distribution.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Complete and execute this cell\n",
    "from scipy import stats\n",
    "dist = # Normal distribution with mean = 0, stdev = 1\n",
    "r = # 10 random draws\n",
    "p = # pdf evaluated at x=0\n",
    "print p,r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment the next line and run\n",
    "# I just want you to know that this magic function exists.\n",
    "#%load code/fig_gaussian_distribution.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note that the convolution of two Gaussians results in a Gaussian.  So $\\mathscr{N}(\\mu,\\sigma)$ convolved with $\\mathscr{N}(\\nu,\\rho)$ is $\\mathscr{N}(\\mu+\\nu,\\sqrt{\\sigma^2+\\rho^2})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gaussian confidence levels\n",
    "\n",
    "The probability of a measurement drawn from a Gaussian distribution that is between $\\mu-a$ and $\\mu+b$ is\n",
    "$$\\int_{\\mu-a}^{\\mu+b} p(x|\\mu,\\sigma) dx.$$\n",
    "For $a=b=1\\sigma$, we get the familar result of 68.3%.  For $a=b=2\\sigma$ it is 95.4%.  So we refer to the range $\\mu \\pm 1\\sigma$ and $\\mu \\pm 2\\sigma$ as the 68% and 95% **confidence limits**, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Can you figure out what the probability is for $-2\\sigma, +4\\sigma$?  Check to see that you get the right answer for the cases above first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Complete and execute this cell\n",
    "N=10000\n",
    "mu=0\n",
    "sigma=1\n",
    "dist = # Complete\n",
    "v = np.linspace( # Complete\n",
    "prob = # Complete\n",
    "print prob.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Log Normal\n",
    "\n",
    "Note that if $x$ is Gaussian distributed with $\\mathscr{N}(\\mu,\\sigma)$, then $y=\\exp(x)$ will have a **log-normal** distribution, where the mean of y is $\\exp(\\mu + \\sigma^2/2)$.  Try it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "x = stats.norm(0,1) # mean = 0, stdev = 1\n",
    "y = np.exp(x)\n",
    "print y.mean()\n",
    "print x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The catch here is that stats.norm(0,1) returns an object and not something that we can just do math on in the expected manner.  What *can* you do with it?  Try dir(x) to get a list of all the methods and properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Complete and execute this cell\n",
    "dist = stats.norm(0,1) # mean = 0, stdev = 1\n",
    "x = # Complete\n",
    "y = # Complete\n",
    "print x.mean(),np.log(y.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $\\chi^2$ Distribution\n",
    "\n",
    "We'll run into the $\\chi^2$ distribution when we talk about Maximum Likelihood in the next chapter.\n",
    "\n",
    "If we have a Gaussian distribution with values ${x_i}$ and we scale and normalize them according to\n",
    "$$z_i = \\frac{x_i-\\mu}{\\sigma},$$\n",
    "then the sum of squares, $Q$ \n",
    "$$Q = \\sum_{i=1}^N z_i^2,$$\n",
    "will follow the $\\chi^2$ distribution.  The *number of degrees of freedom*, $k$ is given by the number of data points, $N$ (minus any constraints).  The pdf of $Q$ given $k$ defines $\\chi^2$ and is given by\n",
    "$$p(Q|k)\\equiv \\chi^2(Q|k) = \\frac{1}{2^{k/2}\\Gamma(k/2)}Q^{k/2-1}\\exp(-Q/2),$$\n",
    "where $Q>0$ and the $\\Gamma$ function would just be the usual factorial function if we were dealing with integers, but here we have half integers.\n",
    "\n",
    "This is ugly, but it is really just a formula like anything else.  Note that the shape of the distribution *only* depends on the sample size $N=k$ and not on $\\mu$ or $\\sigma$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "%run code/fig_chi2_distribution.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Student's $t$ Distribution\n",
    "\n",
    "Another distribution that we'll see later is the Student's $t$ Distribution.\n",
    "\n",
    "If you have a sample of $N$ measurements, $\\{x_i\\}$, drawn from a Gaussian distribution, $\\mathscr{N}(\\mu,\\sigma)$, and you apply the transform\n",
    "$$t = \\frac{\\overline{x}-\\mu}{s/\\sqrt{N}},$$\n",
    "then $t$ will be distributed according to Student's $t$ with the following pdf (for $k$ degrees of freedom): \n",
    "$$p(x|k) = \\frac{\\Gamma(\\frac{k+1}{2})}{\\sqrt{\\pi k} \\Gamma(\\frac{k}{2})} \\left(1+\\frac{x^2}{k}\\right)^{-\\frac{k+1}{2}}$$\n",
    "\n",
    "As with a Gaussian, Student's $t$ is bell shaped, but has \"heavier\" tails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "%run code/fig_student_t_distribution.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What's the point?\n",
    "\n",
    "The point is that we are going to make some measurement.  And we will want to know how likely it is that we would get that measurement in our experiment as compared to random chance.  To determine that we need to know the shape of the distribution.  Let's say that we find that $x=6$.  If our data is $\\chi^2$ distributed with 2 degrees of freedom, then we would integrate the $k=2$ curve above from 6 to $\\infty$ to determine how likely it is that we would have gotten 6 or larger by chance.  If our distribution was instead $t$ distributed, we would get a *very* different answer.  Note that it is important that you decide *ahead of time* what the metric will be for deciding whether this result is significant or not.  More on this later, but see [this article](http://fivethirtyeight.com/features/science-isnt-broken/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Central Limit Theorem\n",
    "\n",
    "One of the reasons that a Gaussian (or Normal) Distribution is so common is because of the **Central Limit Theorem**. It says that for an arbitrary distribution, $h(x)$, that has a well-defined mean, $\\mu$, and standard deviation, $\\sigma$, the mean of $N$ values \\{$x_i$\\} drawn from the distribution will follow a Gaussian Distribution with $\\mathscr{N}(\\mu,\\sigma/\\sqrt{N})$.  (A Cauchy distribution is one example where this fails.)\n",
    "\n",
    "This theorem is the foudation for the performing repeat measurements in order to improve the accuracy of one's experiment.  It is telling us something about the *shape* of the distribution that we get when averaging.  The **Law of Large Numbers** further says that the sample mean will converge to the distribution mean as $N$ increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Personally, I always find this a bit confusing (or at least I forget how it works).  So, let's look at it in detail.\n",
    "Start by plotting a normal distribution with $\\mu=0.5$ and $\\sigma=1/\\sqrt{12}/\\sqrt{2}$.\n",
    "\n",
    "Now take 2 draws from using the `np.random.random` distribution and plot them as a rug plot.  Do that a couple of times (e.g., keep hitting Cntrl-Enter in the cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11922ca50>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAAD8CAYAAACB3pQWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIpJgCnERkQT7f9jvTEIzQMROAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119031290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "N= # Complete\n",
    "mu= # Complete\n",
    "sigma = # Complete\n",
    "\n",
    "u = # Complete\n",
    "dist = # Complete\n",
    "plt.plot(u, # Complete\n",
    "\n",
    "x = # Complete\n",
    "plt.plot(x, 0*x, '|', markersize=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let's average those two draws and plot the result (in the same panel).  Do it as a histogram for 1,000,000 samples (of 2 each).  Use a stepfilled histogram that is normalized with 50% transparency and 100 bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copy your code from above\n",
    "\n",
    "# Add a histogram that is the 2-sample mean of 1,000,000 draws\n",
    "\n",
    "yy = # Complete\n",
    "\n",
    "plt.hist(yy, #Complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now instead of averaging 2 draws, average 3.  Then do it for 10.  Then for 100.  Each time for 1,000,000 samples.\n",
    "Make sure that you adjust the \"expected\" Gaussian based on the number of draws.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Copy your code from above and edit accordingly (or just edit your code from above)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For 100 you will note that your draws are clearly sampling the full range, but the means of those draws are in a *much* more restrictred range.  Moreover they are very closely following a Normal Distribution.  This is the power of the Central Limit Theorem.    We'll see this more later when we talk about **maximum likelihood**.\n",
    "\n",
    "By the way, if your code is ugly, you can run the following cell to reproduce Ivezic, Figure 3.20 which nicely illustrates this in one plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "%run code/fig_central_limit.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If you are confused, then watch this video from the Khan Academy:\n",
    "[https://www.khanacademy.org/math/statistics-probability/sampling-distributions-library/sample-means/v/central-limit-theorem](https://www.khanacademy.org/math/statistics-probability/sampling-distributions-library/sample-means/v/central-limit-theorem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bivariate and Multivariate Distribution Functions\n",
    "\n",
    "Up to now we have been dealing with one-dimensional distribution functions.  Let's now consider a two dimensional distribution $h(x,y)$ where $$\\int_{-\\infty}^{\\infty}dx\\int_{-\\infty}^{\\infty}h(x,y)dy = 1.$$  $h(x,y)$ is telling us the probability that $x$ is between $x$ and $dx$ and *also* that $y$ is between $y$ and $dy$.\n",
    "\n",
    "Then we have the following definitions:\n",
    "\n",
    "$$\\sigma^2_x = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}(x-\\mu_x)^2 h(x,y) dx dy$$\n",
    "\n",
    "$$\\sigma^2_y = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}(y-\\mu_y)^2 h(x,y) dx dy$$\n",
    "\n",
    "$$\\mu_x = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}x h(x,y) dx dy$$\n",
    "\n",
    "$$\\sigma_{xy} = Cov(x,y) = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}(x-\\mu_x) (y-\\mu_y) h(x,y) dx dy$$\n",
    "\n",
    "If $x$ and $y$ are uncorrelated, then we can treat the system as two independent 1-D distributions.  This means that choosing a range on one variable has no effect on the distribution of the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can write a 2-D Gaussian pdf as\n",
    "$$p(x,y|\\mu_x,\\mu_y,\\sigma_x,\\sigma_y,\\sigma_{xy}) = \\frac{1}{2\\pi \\sigma_x \\sigma_y \\sqrt{1-\\rho^2}} \\exp\\left(\\frac{-z^2}{2(1-\\rho^2)}\\right),$$\n",
    "\n",
    "where $$z^2 = \\frac{(x-\\mu_x)^2}{\\sigma_x^2} + \\frac{(y-\\mu_y)^2}{\\sigma_y^2} - 2\\rho\\frac{(x-\\mu_x)(y-\\mu_y)}{\\sigma_x\\sigma_y},$$\n",
    "\n",
    "with $$\\rho = \\frac{\\sigma_{xy}}{\\sigma_x\\sigma_y}$$\n",
    "as the (dimensionless) correlation coefficient.\n",
    "\n",
    "If $x$ and $y$ are perfectly correlated then $\\rho=\\pm1$ and if they are uncorrelated, then $\\rho=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The pdf is now not a histogram, but rather a series of contours in the $x-y$ plane.   These are centered at $(x=\\mu_x, y=\\mu_y)$ and are tilted at angle $\\alpha$, which is given by\n",
    "$$\\tan(2 \\alpha) = 2\\rho\\frac{\\sigma_x\\sigma_y}{\\sigma_x^2-\\sigma_y^2} = 2\\frac{\\sigma_{xy}}{\\sigma_x^2-\\sigma_y^2}.$$\n",
    "\n",
    "For example (Ivezic, Figure 3.22):\n",
    "![Ivezic, Figure 3.22](http://www.astroml.org/_images/fig_bivariate_gaussian_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can define new coordinate axes that are aligned with the minimum and maximum widths of the distribution.  These are called the **principal axes** and are given by\n",
    "$$P_1 = (x-\\mu_x)\\cos\\alpha + (y-\\mu_y)\\sin\\alpha,$$\n",
    "and\n",
    "$$P_2 = -(x-\\mu_x)\\sin\\alpha + (y-\\mu_y)\\cos\\alpha.$$\n",
    "\n",
    "The widths in this coordinate system are\n",
    "$$\\sigma^2_{1,2} = \\frac{\\sigma_x^2+\\sigma_y^2}{2}\\pm\\sqrt{\\left(\\frac{\\sigma_x^2-\\sigma_y^2}{2}\\right)^2 + \\sigma^2_{xy}}.$$\n",
    "\n",
    "Note that the correlation vanishes in this coordinate system and the bivariate Gaussian is just a product of two univariate Gaussians.  This concept will be crucial for understanding Principal Component Analysis when we get to Chapter 7, where PCA extends this idea to even more dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the univariate case we used $\\overline{x}$ and $s$ to *estimate* $\\mu$ and $\\sigma$.  In the bivariate case we estimate 5 parameters: $(\\overline{x},\\overline{y},s_x,s_y,s_{xy})$.  \n",
    "\n",
    "As with the univariate case, it is important to realize that outliers can bias these estimates and that it may be more appropriate to use the median rather than the mean as a more robust estimator for $\\mu_x$ and $\\mu_y$.  Similarly we want robust estimators for the other parameters of the fit.  We won't go into that in detail right now, but see Ivezic, Figure 3.23 for an example:\n",
    "\n",
    "![Ivezic, Figure 3.23](http://www.astroml.org/_images/fig_robust_pca_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For an example of how to generate a bivariate distribution and plot confidence contours, execute the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Base code drawn from Ivezic, Figure 3.22, edited by G. Richards to simplify the example\n",
    "\n",
    "from matplotlib.patches import Ellipse\n",
    "from astroML.stats.random import bivariate_normal\n",
    "from astroML.stats import fit_bivariate_normal\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Create 10,000 points from a multivariate normal distribution\n",
    "mean = [0, 0]\n",
    "cov = [[1, 0.3], [0.3, 1]]\n",
    "x, y = np.random.multivariate_normal(mean, cov, 10000).T\n",
    "\n",
    "# Fit those data with a bivariate normal distribution\n",
    "mean, sigma_x, sigma_y, alpha = fit_bivariate_normal(x,y)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "plt.scatter(x,y,s=2,edgecolor='none')\n",
    "\n",
    "# draw 1, 2, 3-sigma ellipses over the distribution\n",
    "for N in (1, 2, 3):\n",
    "    ax.add_patch(Ellipse(mean, N * sigma_x, N * sigma_y, angle=alpha * 180./np.pi, lw=1, ec='k', fc='none'))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
